In this area of machine learning, we try to understand certain relations beween input-output data. If such relations are established, we then wish to generalize for new \textit{unseen} data.
Things start getting even jucier whenever we wish to take decisions based on new data, resulting in a more generalized task. In this section we explore the formalization provided by Prof. Obozinski.

\begin{enumerate}
    \item \textbf{We have:}
    \begin{itemize}
        \item Data: $\mathcal{D}_n := \left\{ (x_0,y_0), \ldots, (x_n,y_n) \right\}$
        \item i.e. tuples of the form $(x_i, y_i)$
        \item $x_i:=$ input; $y_i:=$ output
    \end{itemize}

    \item \textbf{We want:}
    \begin{itemize}
        \item Given $\mathcal{D}_n$, learn relations of the $x_i$'s with the corresponding $y_i$'s such that we may infer something about a new unseen $y'$ given $x'$.
    \end{itemize}
\end{enumerate}

We now define the two types of tasks cosidered inside supervised learning (amongst others).

\begin{definition}
    A \textit{prediction} task is established to be the discovery of $y`$ (unseen) given $x`$. A \textit{decision} task on the
    other hand, focuses on producing a decision based on $(x`,y`)$ only with the data of $x`$
\end{definition}

For example, take into consideration a medical diagnosis. We have $x_i :=$ patient data i.g. $\left\{ \text{weight}_i, \text{height}_i, \ldots \right\}$; $y_i := \left\{ \text{positive}, \text{negative} \right\}$. Then, a \textbf{prediction task} 
would consist in predicting $y`$ given $x`$. A \textbf{decision task} on the other hand, would then consist on choosing how to treat patient $x`$ i.g. choosing medicine $m \in \left\{ A,B,C \right\}$ (we have to decide on $y`$ by only seeing $x`$).

\vspace{0.3cm}

We now consider the space of all possible decisions; a \textit{learning algorithm }(sometimes called \textit{learning scheme}) $\mathscr{A}$. 

\begin{definition}
    We define a learning algorithm as $$\mathscr{A}: \mathcal{D}_n \rightarrow \hat{f}$$ where $\hat{f}$ is our decision function.
\end{definition}

Obviously we want $\hat{f}$ to be ``good'' (otherwise, \textit{nos estamos haciendo pendejos}). Hence, we must define what it means for $\hat{f}$ to be ``good'' i.e. what we want from $\hat{f}$. 

\begin{definition}
    Let $\mathcal{X}$ be the input space, then, a decision function is defined as $$f:\mathcal{X} \rightarrow \mathcal{A}^{\mathcal{X}}$$ Note that the input space $\mathcal{X}$ is the space of all $x_i$`s.
\end{definition}

Ideally, as stated before, we want a ``good'' function (i.e. decision function) $f$ such that $f(x) \in \mathcal{A}^{\mathcal{X}}$ is ``good'' when compared to an unseen $y$. This means that $f(x)$ must be an accurate prediction of $y$ and it has
the \textbf{smallest possible cost} whenever $y$ occurrs. So, we compute the \textit{loss function} $l$.

\begin{definition}
    Let $\mathcal{Y}$ be the space of all possible outcomes, then $$l:\mathcal{A}^{\mathcal{X}} \times \mathcal{Y} \rightarrow \mathbb{R}$$ defined by $(f(x)=a,y) \mapsto l(a,y)$. Note that this function meassures the cost of taking decision $f$ whenever $y$ occurrs i.e. the \textit{risk}.
\end{definition}

\begin{remark}
    Note that all the above definitions boil down to the fact that we're trying to design a `good' learning algorithm $\mathscr{A}$ that produces $\hat{f}$ in such a way that the risk is minimized. We formalize the definition of a learning algorithm as follows
    $$\mathscr{A}: \left(\mathcal{X} \times \mathcal{Y} \right)^{n} \rightarrow \mathcal{A}^{\mathcal{X}} \text{ given by } \mathcal{D}_n \mapsto \hat{f}$$ Throughout the lecture, unless stated otherwise, we assume that the data is generated by a stochastic process and
    done so i.i.d. as random variables i.e. $(X_i, Y_i)$.
\end{remark}

Because of the fact that this is a statistics class, we'll start getting into the \textit{deets} using much more of their language (statisticians have a fetish for fancy language and syntax). Hence we would like to define what the \textit{expected cost} of takin decision $f$ as the risk $\mathcal{R}$.

\begin{definition}
    We define the risk as follows $$\mathcal{R}(f) \text{ :}= \mathbb{E}\big[l(a,Y) \big] \text{. If  } \exists f^* \in \mathcal{A}^{\mathcal{X}} : \mathcal{R}(f^*)=\text{ inf}_{f \in \mathcal{A}^{\mathcal{X}}} \mathcal{R}(f)$$ then, that $f^*$ is our juicy function we're looking for! statisticians call it the
    \textit{target} function. Now, the \textit{conditional risk} of taking $f$ as an action given $x$ has happened is defined as $$\mathcal{R}(f(x)=a|x) = \mathbb{E}\big[l(a,Y) | X = x \big] = \int l(a,y) dP_{Y|X}(y|x)$$ 
\end{definition}

Note that $dP_{Y|X}(y|x)$ just means we're integrating over the conditional distribution of $Y$ given $X=x$. To simplify further (and remark the fetish statisticians posse), this just means that we're taking average the loss over all possible outcomes of $Y$, weighted by how likely they are given $X=x$.

\begin{remark}
    Note that $$\mathbb{E}\big[ \mathcal{R}(f(X)|X) \big] = \mathbb{E}\big[ \mathbb{E}\big[l(f(X),Y) | X = x \big]\big]=\mathbb{E} \big[l(f(X),Y)\big]$$ i.e. the expected value of $\mathcal{R}(f)$.
\end{remark}

We finally make the last definition of the section, since we're interested in meassuring risks we shall compute the \textit{excess risk} $\varepsilon(f)$ while we're at it. This number tells us how much of our risk is over the optimal ammount.

\begin{definition}
    The excess risk is given by $$\varepsilon (f) := \mathcal{R}(f) - \mathcal{R}(f^*) = \mathbb{E} \big[l(f(X),Y)\big] - \mathbb{E} \big[l(f^*(X),Y)\big]$$ $$\Rightarrow \varepsilon (f) = \mathbb{E} \big[l(f(X),Y) - l(f^*(X),Y)\big]$$
\end{definition}

This is a great book!\cite{judson2019abstract}.