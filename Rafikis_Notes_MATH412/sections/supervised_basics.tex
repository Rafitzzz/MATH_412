In this area of machine learning, we try to understand certain relations beween input-output data. If such relations are established, we then wish to generalize for new \textit{unseen} data.
Things start getting even jucier whenever we wish to take decisions based on new data, resulting in a more generalized task. In this section we explore the formalization provided by Prof. Zemel.

\begin{enumerate}
    \item \textbf{We have:}
    \begin{itemize}
        \item Data: $\mathcal{D}_n := \left\{ (x_0,y_0), \ldots, (x_n,y_n) \right\}$
        \item i.e. tuples of the form $(x_i, y_i)$
        \item $x_i:=$ input; $y_i:=$ output
    \end{itemize}

    \item \textbf{We want:}
    \begin{itemize}
        \item Given $\mathcal{D}_n$, learn relations of the $x_i$'s with the corresponding $y_i$'s such that we may infer something about a new unseen $y'$ given $x'$.
    \end{itemize}
\end{enumerate}

We now define the two types of tasks cosidered inside supervised learning (amongst others).

\begin{definition}
    A \textit{prediction} task is established to be the discovery of $y`$ (unseen) given $x`$. A \textit{decision} task on the
    other hand, focuses on producing a decision based on $(x`,y`)$ only with the data of $x`$
\end{definition}

For example, take into consideration a medical diagnosis. We have $x_i :=$ patient data i.g. $\left\{ \text{weight}_i, \text{height}_i, \ldots \right\}$; $y_i := \left\{ \text{positive}, \text{negative} \right\}$. Then, a \textbf{prediction task} 
would consist in predicting $y`$ given $x`$. A \textbf{decision task} on the other hand, would then consist on choosing how to treat patient $x`$ i.g. choosing medicine $m \in \left\{ A,B,C \right\}$ (we have to decide on $y`$ by only seeing $x`$).

\vspace{0.3cm}

We now consider the space of all possible decisions; a \textit{learning algorithm }(sometimes called \textit{learning scheme}) $\mathscr{A}$. 

\begin{definition}
    We define a learning algorithm as $$\mathscr{A}: \mathcal{D}_n \rightarrow \hat{f}$$ where $\hat{f}$ is our decision function.
\end{definition}

Obviously we want $\hat{f}$ to be ``good'' (otherwise, \textit{nos estamos haciendo pendejos}). Hence, we must define what it means for $\hat{f}$ to be ``good'' i.e. what we want from $\hat{f}$. 

\begin{definition}
    Let $\mathcal{X}$ be the input space, then, a decision function is defined as $$f:\mathcal{X} \rightarrow \mathcal{A}^{\mathcal{X}}$$ Note that the input space $\mathcal{X}$ is the space of all $x_i$`s.
\end{definition}

Ideally, as stated before, we want a ``good'' function (i.e. decision function) $f$ such that $f(x) \in \mathcal{A}^{\mathcal{X}}$ is ``good'' when compared to an unseen $y$. This means that $f(x)$ must be an accurate prediction of $y$ and it has
the \textbf{smallest possible cost} whenever $y$ occurrs. So, we compute the \textit{loss function} $l$.

\begin{definition}
    Let $\mathcal{Y}$ be the space of all possible outcomes, then $$l:\mathcal{A}^{\mathcal{X}} \times \mathcal{Y} \rightarrow \mathbb{R}$$ defined by $(f(x)=a,y) \mapsto l(a,y)$. Note that this function meassures the cost of taking decision $f$ whenever $y$ occurrs i.e. the \textit{risk}.
\end{definition}

\begin{remark}
    Note that all the above definitions boil down to the fact that we're trying to design a `good' learning algorithm $\mathscr{A}$ that produces $\hat{f}$ in such a way that the risk is minimized. We formalize the definition of a learning algorithm as follows
    $$\mathscr{A}: \left(\mathcal{X} \times \mathcal{Y} \right)^{n} \rightarrow \mathcal{A}^{\mathcal{X}} \text{ given by } \mathcal{D}_n \mapsto \hat{f}$$ Throughout the lecture, unless stated otherwise, we assume that the data is generated by a stochastic process and
    done so i.i.d. as random variables i.e. $(X_i, Y_i)$.
\end{remark}

Because of the fact that this is a statistics class, we'll start getting into the \textit{deets} using much more of their language (statisticians have a fetish for fancy language and syntax). Hence we would like to define what the \textit{expected cost} of taking decision $f$ as the risk $\mathcal{R}$.

\begin{definition}
    We define the risk as follows $$\mathcal{R}(f) \text{ :}= \mathbb{E}\big[l(a,Y) \big] \text{. If  } \exists f^* \in \mathcal{A}^{\mathcal{X}} : \mathcal{R}(f^*)=\text{ inf}_{f \in \mathcal{A}^{\mathcal{X}}} \mathcal{R}(f)$$ then, that $f^*$ is our juicy function we're looking for! statisticians call it the
    \textit{target} function. Now, the \textit{conditional risk} of taking $f$ as an action given $x$ has happened is defined as $$\mathcal{R}(f(x)=a|x) = \mathbb{E}\big[l(a,Y) | X = x \big] = \int l(a,y) dP_{Y|X}(y|x)$$ 
\end{definition}

Note that $dP_{Y|X}(y|x)$ just means we're integrating over the conditional distribution of $Y$ given $X=x$. To simplify further (and remark the fetish statisticians posse), this just means that we're taking average the loss over all possible outcomes of $Y$, weighted by how likely they are given $X=x$.

\begin{remark}
    Note that $$\mathbb{E}\big[ \mathcal{R}(f(X)|X) \big] = \mathbb{E}\big[ \mathbb{E}\big[l(f(X),Y) | X = x \big]\big]=\mathbb{E} \big[l(f(X),Y)\big]$$ i.e. the expected value of $\mathcal{R}(f)$.
\end{remark}

We finally make the last definition of the section, since we're interested in meassuring risks we shall compute the \textit{excess risk} $\varepsilon(f)$ while we're at it. This number tells us how much of our risk is over the optimal ammount.

\begin{definition}
    The excess risk is given by $$\varepsilon (f) := \mathcal{R}(f) - \mathcal{R}(f^*) = \mathbb{E} \big[l(f(X),Y)\big] - \mathbb{E} \big[l(f^*(X),Y)\big]$$ $$\Rightarrow \varepsilon (f) = \mathbb{E} \big[l(f(X),Y) - l(f^*(X),Y)\big]$$
\end{definition}

\subsection{Examples}
As for every new concept, one must get their hands dirty in order to fully grasp the idea that is being transmitted. We explore some basic examples as to how these things might be used in practice by taking a look at two main cases: \textit{Ordinary Least Squares} and \textit{Classification}. Our main goal is
to derived the target function $f^*$ given a loss function $l$.

\subsubsection{Ordinary Least Squirts Regression (OLS)}
Consider the case where both our action space $\mathcal{A}^{\mathcal{X}}$ and our output space $\mathcal{Y}$ are both the real numbers $\mathbb{R}$. The loss function in this case is then defined as $l:\mathbb{R}^2 \rightarrow \mathbb{R}$ and it is given by $l(f(x)=a,y)=(a-y)^2$. Then, the expected cost of taking decision $f$
is the following $$\mathcal{R}(f) = \mathbb{E}\big[l(a,y)\big] = \mathbb{E}\big[(a-y)^2\big] = \mathbb{E}\big[(y-a)^2\big]$$ 

We now make an educated guess (cheeky little trick) and assume that $\hat{f} = \mathbb{E}\big[Y|X\big]$ and so, we consider the following expected conditional risk:
$$\mathcal{R}(f(X)|X) = \mathbb{E}\big[l(a,y)|X\big] = \mathbb{E}\big[(Y-f(X))^2|X\big] = \mathbb{E}\big[(Y-\hat{f}(X)+\hat{f}(X)-f(X))^2|X\big]$$ 

Note that we also implemented a cheeky trick, it's just adding a glorified zero into the mix i.e. $-\hat{f}(X)+f^*(X) = 0$. Expanding the quadratic term inside the expectation, we obtain the following equation:
$$\mathbb{E}\big[(Y-\hat{f}(X))^2 + 2\big(Y-\hat{f}(X)\big)\big(\hat{f}(X)-f(X)\big) + (\hat{f}(X)-f(X))^2 | X\big]$$
$$= \mathbb{E}\big[(Y-\hat{f}(X))^2 | X\big] + \mathbb{E}\big[2\big(Y-\hat{f}(X)\big)\big(\hat{f}(X)-f(X)\big) | X\big] + \mathbb{E}\big[(F^*(X)-F(X))^2 | X\big]$$
$$= \mathbb{E}\big[(Y-\hat{f}(X))^2 | X\big] + 2\big(\hat{f}(X)-f(X)\big) \mathbb{E}\big[Y-\hat{f}(X) | X\big] + \mathbb{E}\big[(\hat{f}(X)-f(X))^2 | X\big]$$

\begin{remark}
    Note that $2\big(\hat{f}(X)-f(X)\big) \mathbb{E}\big[Y-\hat{f}(X) | X\big] = 0$ since $$\mathbb{E}\big[Y-\hat{f}(X) | X\big]=\mathbb{E}\big[Y-\mathbb{E}\big[Y|X\big] | X\big] = \mathbb{E}\big[Y|X\big] - \mathbb{E}\big[Y|X\big] = 0$$
\end{remark}

So, we get that the conditional risk:
$$\mathcal{R}(f(X)|X) = \mathbb{E}\big[(Y-\hat{f}(X))^2 | X\big] + \mathbb{E}\big[(\hat{f}(X)-f(X))^2 | X\big]$$
$$\Rightarrow \mathcal{R}(f(X)|X) = \mathcal{R}(\hat{f}(X)|X) + \mathbb{E}\big[(\hat{f}(X)-f(X))^2 | X\big]$$

Now, let's think for a moment. We essentially want to find $f^*$ (target function) such that our risk $\mathcal{R}(f(X)|X)$ is minimized whenever we take that action. 

\vspace{0.2cm}

Since $\mathcal{R}(\hat{f}(X)|X)$ can't help us, we turn our but cheeks to the other term, i.e. $\mathbb{E}\big[(\hat{f}(X)-f(X))^2 | X\big]$. Note that if $f=\hat{f}$, this term goes to zero, and hence our conditional risk is minimized. Voila! since there actually exists such function that minimizes our risk, we've arrived at our sensual (target) function babyyy.

$$ \therefore \text{ }f^*=\hat{f} $$

\subsubsection{Classification}
We now consider a classification problem where $\mathcal{A}^{\mathcal{X}} = \mathcal{Y} := \{0, \ldots, K-1\}$ and the loss function is given by the 0-1 loss function i.e. $l(a,y)=\mathbbm{1}_{\{a\neq y\}}$. Then, the risk of $f$ is given by the equation
$$\mathcal{R}(f) = \mathbb{E}\big[l(f(x)=a,y)\big] = \mathbb{P}\big(f(X) \neq Y\big) = 1 - \mathbb{P}\big(f(X) = Y\big)$$

Now, if we consider the conditional risk, we obtain the following derivation
$$\mathcal{R}(f|X=x) = \mathbb{E}\big[l(f(x)=a,y)|X=x\big] = \mathbb{P}\big(f(x) \neq Y|X=x\big) = 1 - \mathbb{P}\big(f(x) = Y|X=x\big)$$

Think for a moment. This states that in order to minimize $\mathcal{R}(f|X=x)$ we must then find the highest possible value for $\mathbb{P}\big(f(x) = Y|X=x\big) = \mathbb{P}\big(Y = f(x)|X=x\big)$. So, the following equivalence holds
$$\text{min}\mathcal{R}(f|X=x) = \text{max}_{f\in \mathcal{A}^{\mathcal{X}}}\mathbb{P}\big(Y = f(x)|X=x\big)$$
$$\Rightarrow \text{arg max}_{k\in K}\mathbb{P}\big(Y = k|X=x\big) = f^*$$

And so, we've found our hard-to-get (like women playing their games) target function. That last implication basically states that we must do the following: \textit{for each value of x, pick y that makes it the largest possible value}.

This is a great book!\cite{judson2019abstract}